{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from functools import reduce\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "nlp = spacy.load(\"nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "for entry in os.scandir(\"../data/stimuleringsregeling/\"):\n",
    "    if entry.is_file():\n",
    "        with open(entry.path) as source_file:\n",
    "            source = json.load(source_file)\n",
    "        if any(doc for doc in source.get(\"documents\", []) if doc.get(\"text\", None)):\n",
    "            sources.append(source)\n",
    "\n",
    "source_documents = [doc[\"text\"] for source in sources for doc in source[\"documents\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [nlp(doc) for doc in source_documents if len(doc) < 1000000]  # text size limit comes from spaCy\n",
    "\n",
    "documents_count = len(documents)\n",
    "token_count = reduce(lambda count, doc: count+len(doc), documents, 0)\n",
    "sentence_count = reduce(lambda count, doc: count+len(list(doc.sents)), documents, 0)\n",
    "entity_count = reduce(\n",
    "    lambda count, doc: count+len([\n",
    "        ent for ent in doc.ents \n",
    "        if len(ent.text.strip())  # not sure why there are empty entities from spaCy\n",
    "    ]),  \n",
    "    documents, \n",
    "    0\n",
    ")\n",
    "\n",
    "print(\"Total sources: {}\".format(documents_count))\n",
    "print(\"Average document length: {}\".format(token_count / documents_count))\n",
    "print(\"Average sentences: {}\".format(sentence_count / documents_count))\n",
    "print(\"Average entities: {}\".format(entity_count / documents_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = documents[0]\n",
    "print(list(doc.sents))\n",
    "print([ent for ent in doc.ents if len(ent.text.strip())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(source_documents)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_by_keywords_tfidf = []\n",
    "\n",
    "for source in sources:\n",
    "    \n",
    "    keywords = source.get(\"keywords\", [])\n",
    "    if not keywords:\n",
    "        continue\n",
    "\n",
    "    documents = source.get(\"documents\")\n",
    "    vectors = tfidf_vectorizer.transform([doc[\"text\"] for doc in documents])\n",
    "    source_vector = vectors.sum(axis=0).A1\n",
    "    source[\"keywords_tfidf\"] = {\n",
    "        word.lower(): source_vector[ \n",
    "            tfidf_vectorizer.vocabulary_[word.lower()]  # at index for the word\n",
    "        ]\n",
    "        for keyword in keywords\n",
    "        for word in keyword.split(\" \")  # splitting on spaces because we're limited to unigram for now\n",
    "        if word in tfidf_vectorizer.vocabulary_  # 'e-module' was not included in vocab??\n",
    "    }\n",
    "\n",
    "    keywords_tfidf_values = list(source[\"keywords_tfidf\"].values())\n",
    "    if not keywords_tfidf_values:  # for sources with keywords only outside of vocab :(\n",
    "        continue\n",
    "    keywords_tfidf_score = sum(keywords_tfidf_values) / len(keywords_tfidf_values)\n",
    "    sources_by_keywords_tfidf.append((keywords_tfidf_score, source))\n",
    "\n",
    "sources_by_keywords_tfidf.sort(key=lambda element: element[0], reverse=True)\n",
    "target_sources = sources_by_keywords_tfidf[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sources_ids = set([source[\"id\"] for _, source in target_sources])\n",
    "\n",
    "for keywords_tfidf, source in target_sources:\n",
    "    \n",
    "    if source[\"id\"] not in target_sources_ids:\n",
    "        continue\n",
    "    \n",
    "    documents = source.get(\"documents\")\n",
    "    noun_documents = []\n",
    "    for doc in documents:\n",
    "        nouns = [token.text for token in nlp(doc[\"text\"]) if token.pos == spacy.parts_of_speech.NOUN] \n",
    "        noun_documents.append(\" \".join(nouns))\n",
    "    vectors = tfidf_vectorizer.transform(noun_documents)\n",
    "    source_vector = vectors.sum(axis=0).A1\n",
    "    highest_tfidf_indexes = source_vector.argsort()[-10:]\n",
    "\n",
    "    source[\"noun_tfidf\"] = {\n",
    "        feature_names[ix]: source_vector[ix]\n",
    "        for ix in highest_tfidf_indexes\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/tfidf-keywords.json\", \"w\") as json_file:\n",
    "    json.dump(target_sources, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:surf-test]",
   "language": "python",
   "name": "conda-env-surf-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
